\section{Learned Hierarichal Networks}
\label{sec:fidler}
\todo[inline, author=Michael]{Draft. Please add some comments in todos.. need feedback}
This section aims to explain the work presented in \citet{fidler2009learning}. 
There it is proposed to use hierarchical compositionality (explained in section \ref{sec:hierarchical-comp}) for object recognition. 

\subsection{Hierarchical compositionality}
\label{sec:hierarchical-comp}
In essence, the term compositionality refers to complex parts made from combining other parts \citep[p. 2]{fidler2007towards}.
%\todo[inline, author=Michael]{Here is a tangent: 
The term is used within mathematics and languages as well, but in this paper,
only its use with respect to extracting visual features is discussed. 
%} 

An example of combining multiple parts into a more complex part is shown in figure \ref{fig:compositionality1}. 
The letter "T" is formed from or \textit{composed} by two simple line fragments. 
%Of course it would only make sense to compose a part from a group of parts if they are related. 
How to determine which parts to combine into a more complex part will be discussed below.

\begin{figure}[h!] %compositionality1
\centering
\includegraphics[width=0.3\textwidth]{graphics/compositionality1}
\caption[Compositionality]{Combining two simple features into a more complex feature/composition.}
\label{fig:compositionality1}
\end{figure}

In \citet{fidler2009learning}, a hierarchy is built,
combining features in a lower layer into more complex features in the layer above it. 
The motivation for using a hierarchy has already been discussed in section \ref{sec:deep_hierarchies}. 
Three different types of layers are used: 
\todo[inline, author=Michael]{I need a graphic here.. }

\paragraph*{The bottom layer} extracts simple features (edges) from the image.
It uses a filter bank\footnote{A filter bank is an array of filters that split the signal into different components \citep{ref:filterbank}.}
consisting of 6 different Gabor filters of different
orientation.\footnote{A Gabor filter is a combination of a Sine and a Gaussian distribution that is often used within computer vision for edge detection \citep{kamarainen2012gabor}.} 
The filter bank is shown in figure \ref{fig:filterbank}. 
This filter bank is applied directly to the image and the most prominent features are extracted. 
This is done by comparing the response of the filters with a threshold value,
and thereby the sensitivity and number of extracted features can be controlled.
This is repeated for different image scales, to make the system robust towards the objects being of different sizes (scale-invariance).

The feature extraction/detection done at this layer is similar to area V1 of the primate visual cortex, see section \ref{sec:pvc}. 

\begin{figure}[h!] %filterbank  %zoom = 400%
\centering
\includegraphics[width=0.4\textwidth]{graphics/layer1_features}
\caption[Filter bank used for feature extraction]{Filter bank used for feature extraction at layer 1 \cite[fig.~7]{fidler2009learning}.}
\label{fig:filterbank}
\end{figure}

\paragraph*{Category independent layers} are located just above the bottom layer, and are learned without supervision.
In \citet{fidler2009learning}, layers 2 and 3 are category independent layers. 
The features of a layer are learned by combining parts from the layer below it
(ie. layer 2 is learned from layer 1 features, layer 3 from layer 2 features etc.).
This is described in section \ref{sec:composition-of-parts}.
Examples of of layer 2 and 3 features are shown in figure \ref{fig:layer2+3}.

The learning of these layers is unsupervised
- this means that no input from humans is necessary when the layers are learned,
and that the layers are learned from a set of unlabeled images containing a variety of objects. 

\begin{figure}[h!] %layer2 + 3  %zoom = 400%
	\centering
\begin{subfigure}[b]{0.3\textwidth}
	\includegraphics[height=4cm]{graphics/layer2_features}
	%\caption{blabla}
\end{subfigure}
\hspace{1cm}
\begin{subfigure}[b]{0.3\textwidth}
	\includegraphics[height=4cm]{graphics/layer3_features}
	%\caption{blabla}
\end{subfigure}
\caption[Learned features in layers 2 and 3]{Learned features in layers 2 and 3 \citep[fig.~7]{fidler2009learning}.}
\label{fig:layer2+3}
\end{figure}

\paragraph*{Category specific layers} are the highest layers (4 and 5) of the hierarchy.
These layers are learned from images of specific categories.
The top-most layer combines the parts through the center of the object, to form the final object. 

\subsubsection{Composition of parts}
\label{sec:composition-of-parts}
Every part $\mathcal{P}_{\ell}^n$  in $\mathcal{L}_n$ (layer n) is composed of parts in $\mathcal{L}_{n-1}$.
This means that all parts in $\mathcal{L}_5$ are compositions of parts in $\mathcal{L}_4$ and so forth.
The composite part is made of a central part and its neighbours in the layer below. 
This list of sub-parts is shown in equation \ref{eqn:composition_of_parts}. 

\begin{equation}
\mathcal{P}_{\ell}^n = \left(\mathcal{P}_{central}^{n-1} , \left\{\left(\mathcal{P}_j^{n-1}, \mu_j, \Sigma_j \right) \right\}_j \right)
\label{eqn:composition_of_parts}
\end{equation}
where $\mu = (x_j, y_j)$ is the relative position of sub-part $\mathcal{P}_{j}^{n-1}$. 
$\Sigma_j$ is the max allowed variance of its position around $(x_j, y_j)$.
An example of a composite part is shown in figure \ref{fig:compositionality2} below.

\begin{figure}[h!] %composing part in layer 3
\centering
\includegraphics[scale=0.7]{graphics/compositionality2}
\caption[Example of a Layer 3 part]{Example of a Layer 3 part, made from three Layer 2 parts \citep[fig.~2]{fidler2009learning}.}
\label{fig:compositionality2}
\end{figure}

To avoid (nearly) identical parts being learned multiple times in different compositions,
the learned compositions are grouped, based on similarity and co-occurrence. 
Compositions made up from the same sub-parts, but with different central parts,
are grouped together. Parts often occurring together are also grouped.
This grouping of parts lowers the number of parts in each layer,
and makes the hierarchical representation more efficient.

\subsubsection{Indexing and matching}
\label{sec:indexing-matching}
When the hierarchy is built,
recognition of the items is done using an indexing and matching scheme.
Recognizing objects in an image is done in the following steps: 

\begin{enumerate}
	\item Extract $\mathcal{L}_1$ features by applying the filter bank.
A list of filters producing the maximum response and their position is saved as $\mathcal{L}_1$ parts. 
	\item Every found feature is checked against the parts found in the layer above. 
	Every part in $\mathcal{L}_{N}$  with part $\mathcal{P}_{\ell_k}^{n-1}$ as the central part is \textit{indexed},
yielding constant lookup time. 
	\item The parts $\mathcal{P}_{\ell}^{n}$ having part $\mathcal{P}_{\ell_k}^{n-1}$ as center
are then \textit{matched}. In practice, this is done by checking if all subparts are present in layer $\mathcal{L}_{N-1}$.
	\item Repeat step 2 + 3 for all remaining layers.  
\end{enumerate}

The process is also shown in figure \ref{fig:indexing-matching}. 
\begin{figure}[h!] %composing part in layer 3
\centering
\includegraphics[width=0.8\textwidth]{graphics/indexing-matching}
\caption[Indexing and matching]{The left side shows how a found feature $\mathcal{P}^{n-1}$ is checked against
parts in $\mathcal{L}_{N-1}$, where it is the central part. 
On the right side, the found part $\mathcal{P}_{3}^{n}$ is matched against the neighbour features in $\mathcal{L}_{N-1}$. 
If all subparts are found, the composite part is confirmed and added to the list of found $\mathcal{L}_{N}$ features. \\
\citep[fig.~3]{fidler2009learning}.}
\label{fig:indexing-matching}
\end{figure}

\subsection{Validation}
\label{sec:fidler-results}
To validate the approach presented, learning of the hierarchy was tested by \citet{fidler2009learning},
using a set of 1500 images. 
The unsupervised part of the learned hierarchy consisted of 160 parts in Layer 2 and 553 parts in Layer 3. 
Some of the learned features are shown in figure \ref{fig:layer2+3}.

The category specific layers were also tested. Multiple categories were learned, one of them being faces. 
The faces category was learned by using 20 images containing faces.
Parts in $\mathcal{L}_{4}$ were then used to learn $\mathcal{L}_{5}$ by looking at the parts relative to the center of the face. 
The learned features for faces are shown in figure \ref{fig:layer4-5}.

\begin{figure}[h!] %faces layer 4 + 5
\centering
\includegraphics[scale=0.7]{graphics/layer4_5_faces}
\caption[Learned parts in $\mathcal{L}_{4}$ and $\mathcal{L}_{5}$ for recognizing faces]{Learned parts in $\mathcal{L}_{4}$ and $\mathcal{L}_{5}$ for recognizing faces \citep[fig.~10]{fidler2009learning}.}
\label{fig:layer4-5}
\end{figure}
